{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load in IMDB dataset\n",
    "df = pd.read_csv('./Task2-Sentiment_Analysis_Using_Neural_Networks/data/imdb_labelled.txt', sep='\\t+', header=None, names=['review', 'sentiment'], engine='python')\n",
    "\n",
    "# Create a function to split data into train, validation, and test sets\n",
    "def train_val_test_split(X, y, val_size=0.10, test_size=0.10, rand_seed=42):\n",
    "    \"\"\"\n",
    "    Splits data into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.Series): Series of features\n",
    "        y (pd.Series): Series of labels\n",
    "        val_size (float): Proportion of data to use for validation set\n",
    "        test_size (float): Proportion of data to use for test set\n",
    "        rand_seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Six pd.Series objects: X_train, X_val, X_test, y_train, y_val, y_test corresponding to\n",
    "        the train, validation, and test sets for the features and labels respectively.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({X.name: X, y.name: y})\n",
    "    \n",
    "    val_int = int(val_size*len(df))\n",
    "    test_int = int(test_size*len(df))\n",
    "    \n",
    "    train_split = len(df) - val_int - test_int\n",
    "    val_split = len(df) - test_int\n",
    "    \n",
    "    shuf = df.sample(frac=1, random_state=rand_seed) # Shuffle rows\n",
    "    \n",
    "    X_train = shuf.iloc[:train_split][X.name]\n",
    "    X_val = shuf.iloc[train_split:val_split][X.name]\n",
    "    X_test = shuf.iloc[val_split:][X.name]\n",
    "    y_train = shuf.iloc[:train_split][y.name]\n",
    "    y_val = shuf.iloc[train_split:val_split][y.name]\n",
    "    y_test = shuf.iloc[val_split:][y.name]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df.review, df.sentiment)\n",
    "\n",
    "# Use TextVectorization to create an input layer for the model that converts text to sequences of integers\n",
    "vectorizer = tf.keras.layers.TextVectorization(max_tokens=5000, output_sequence_length=250)\n",
    "vectorizer.adapt(X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Vocab size : {vectorizer.vocabulary_size()}')\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "print(f\"Top 20 vocab items: {vocab[:20]}\")\n",
    "sample_text = [\"This movie was fantastic!\"]\n",
    "sample_text_vectorized = vectorizer(sample_text)\n",
    "print(f\"Sample Text: {sample_text}\")\n",
    "print(f\"Vectorized Text: {sample_text_vectorized}\")\n",
    "print(f\"Tokens: {[vocab[word_index] for word_index in sample_text_vectorized[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build and compile the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(vectorizer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different model architecture including an Embedding layer\n",
    "# as well as using an LSTM layer\n",
    "\n",
    "# model2 = tf.keras.Sequential([\n",
    "#     vectorizer,\n",
    "#     tf.keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero=True),\n",
    "#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "\n",
    "model2 = tf.keras.Sequential()\n",
    "model2.add(vectorizer)\n",
    "model2.add(tf.keras.layers.Embedding(input_dim=len(vectorizer.get_vocabulary()), output_dim=64, mask_zero=True))\n",
    "model2.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.2)))\n",
    "model2.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.2)))\n",
    "model2.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves for loss and accuracy on the same plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.title(\"Loss - Training vs. Validation\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history[\"accuracy\"])\n",
    "    plt.plot(history.history[\"val_accuracy\"])\n",
    "    plt.title(\"Accuracy - Training vs. Validation\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Training\", \"Validation\"])\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(model2.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix using numpy and pandas\n",
    "# Plot confusion matrix with Seaborn heatmap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model2.predict(X_test.values)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "y_pred = y_pred.reshape(1, -1)[0]\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    data = {'y_Actual': y_true, 'y_Predicted': y_pred}\n",
    "    df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "    confusion_matrix = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cmatrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cmatrix, annot=True)\n",
    "    plt.title('Model Predictions Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
